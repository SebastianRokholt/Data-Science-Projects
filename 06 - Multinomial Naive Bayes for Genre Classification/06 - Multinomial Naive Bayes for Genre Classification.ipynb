{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a504a731",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes for classifying text to movie genres\n",
    "### An assignment as part of the course INF367: Selected Topics in Artificial Intelligence at the University of Bergen\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Description of the problem\n",
    "In the field of natural language processing, we often want to assign a category (label) to text documents depending on their content. The Naive Bayes classifiers are simple approaches to solving the text categorisation problem that often perform suprisingly well. They rely on a simple bag-of-words representation of the document, and make use of Bayes' rule: \n",
    "\n",
    "$\\Large P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$\n",
    "\n",
    "For classifying text documents, the formula can be distilled into taking the maximum of a likelihood and a prior:\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/naive_bayes_argmax_formula.png\" width=\"400\"/>\n",
    "</div>\n",
    "<div align=\"center\"><i><b>Fig. 1</b>: The Naive Bayes argmax formula for classification.<br></i></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The Multinomial Naive Bayes algorithm makes two important assumptions: \n",
    "1. The position of words in the document doesn't matter\n",
    "2. The feature probabilities are independent.\n",
    "These assumptions ensure that calculating the probabilities is simple and efficient, though the accuracy of a Naive Bayes classifier is generally lower than for more complicated algorithms. \n",
    "\n",
    "The rest of part 1 of this report will aim to provide and describe the implementation of the Multinomial Naive Bayes approach. \n",
    "\n",
    "\n",
    "### Description of the approach\n",
    "I decided to use the algorithm described on page 62 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf), Third Edition, by Daniel Jurafsky and James H. Martin, as a guideline for my implementation of Multinomial Naive Bayes. The algorithm can be seen in its entirety below:\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/naive_bayes_alg.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\"><i><b>Fig. 2</b>: The Multinomial Naive Bayes Algorithm. (Jurafsky & Martin, 2022)<br></i></div>\n",
    "\n",
    "It is worthy to note that the algorithm calculates the sum of the logarithm of the probabilities, instead of simply multiplying them according to the formula. This is because multiplying many low probabilities can cause a floating-point underflow, so using the logs makes things more computationally efficient.\n",
    "\n",
    "I implemented the algorithm to solve the problem of classifying which genre a movie belongs to, based on some text document describing the movie. The document can be any related text such as reviews, a synopsis or the movie's English subtitles. The aim was to be able to classify the simple document `D = fast, couple, shoot, fly` with the label `action`. The classifier was trained with a tiny dataset, consisting of five documents with 3-5 keywords and a label of either `comedy` or `action`. \n",
    "\n",
    "\n",
    "\n",
    "### Description of the software\n",
    "There are two ways to run the software: <br>\n",
    "1. **As part of this Jupyter Notebook.** <br>\n",
    "Open this file in Jupyter Notebook or an IDE capable of running notebooks (e.g. PyCharm). Select \"Kernel\" from the top menu, then \"Restart and Run All\" to run the notebook in your local environment. \n",
    "Since there are no dependencies required to run the code, re-use through copy-pasting the code is trivial. Simply copy-paste the code over to your Python file and you should be able to run it. \n",
    "2. **By pip installation** <br>\n",
    "A directory called `nb_sgd_classify` has been provided along with this notebook. To install the software, simply download the folder, navigate to it in the terminal, and run the command `python3 -m pip install .` (punctuation included. Indicates that pip should install the current directory.) This will install both the software package and required dependencies (numpy), unless these are already installed. Then you may use the SGD and Naive Bayes classes as you wish by importing them as usual. See the `test` directory for example usage. \n",
    "<br><br>\n",
    "\n",
    "The classifier is implemented as a Python class with three methods: \n",
    "1. `set_feature_freq_per_class`: Used to extract the class of each word in the training data and count their frequency. \n",
    "2. `train`: Calculates the a priori probabilities for each class and the log likelihood of each feature (word). Stores these values as class attributes to use for testing new documents. \n",
    "3. `test`: Predicts the class of a single document. Calculates the sum of the log likelihoods for each class for each feature (word) in the test document, and returns the class with the highest sum of log likelihoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96336e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    \"\"\"\n",
    "    A class for creating and testing a Multinomial Naive Bayes classifier\n",
    "    with optional add-k smoothing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classes = []  # The classification labels\n",
    "        self.class_feature_freq = defaultdict(dict)  # The number of times each feature (word) occurs for each class\n",
    "        self.class_total_feature_freq = {}  # The total number of features per class\n",
    "        self.total_doc_count = 0  # The total number of documents in the training data\n",
    "        self.priorlogs = {}  # The a priori probabilities by class\n",
    "        self.feature_ll = defaultdict(dict)  # The log likelihoods for each feature\n",
    "        self.sum_ll = {}  # The test document's cumulative log likelihood ratios for each class\n",
    "\n",
    "    def set_feature_freq_per_class(self, X_train: list, y_train: list, k: int = 1):\n",
    "        \"\"\"\n",
    "        Extracts the class for each word in the training data, counts their frequency\n",
    "        and sets the class_feature_freq attribute accordingly.\n",
    "        :param X_train: The training data (documents). A list of lists.\n",
    "        :param y_train: The training data classification labels.\n",
    "        :param k: The add-k smoothing parameter. Default is 1.\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop over the training examples and update the frequency of each word for each class\n",
    "        for doc, label in zip(X_train, y_train):\n",
    "            for word in doc:\n",
    "                if word in self.class_feature_freq[label].keys():\n",
    "                    self.class_feature_freq[label][word] += 1\n",
    "                else:\n",
    "                    # If it is a new word for the current class, add it to the frequency dict with an initial value\n",
    "                    self.class_feature_freq[label][word] = 1 + k\n",
    "\n",
    "    def train(self, X_train: list, y_train: list, k: int = 1):\n",
    "        \"\"\"\n",
    "        Fits the Naive Bayes model to the training data. \n",
    "        An implementation inspired by the Multinomial Naive Bayes algorithm presented in\n",
    "        Speech and Language Processing (Jurafsky & Martin, 2022), page 62, Figure 4.2.\n",
    "        :param X_train: The training data features. A list of lists containing the tokenized and lemmatized documents.\n",
    "        :param y_train: The training data labels.\n",
    "        :param k: The parameter for add-k smoothing (optional).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve unique classes in the training data\n",
    "        self.classes = list(set(y_train))\n",
    "        print(\"Unique classes: \", self.classes)\n",
    "\n",
    "        # Calculates the total number of documents\n",
    "        self.total_doc_count = len(X_train)\n",
    "        print(f\"Total number of documents: \", self.total_doc_count)\n",
    "\n",
    "        # Calculates the frequency of each word per class in the training data\n",
    "        self.set_feature_freq_per_class(X_train, y_train, k)\n",
    "        print(\"Training sample counts per class: \", self.class_feature_freq.items())\n",
    "\n",
    "        for label in self.classes:\n",
    "            # Calculates the total number of features per class\n",
    "            self.class_total_feature_freq[label] = sum(self.class_feature_freq[label].values())\n",
    "            print(f\"{label} total feature counts:\", self.class_total_feature_freq[label])\n",
    "            # Calculates the log a priori probabilities by class\n",
    "            self.priorlogs[label] = math.log(self.class_total_feature_freq[label] / self.total_doc_count)\n",
    "            print(f\"Log prior for {label}: \", self.priorlogs[label])\n",
    "\n",
    "            # Calculates the log likelihood of each feature (word) for each class.\n",
    "            for word, count in self.class_feature_freq[label].items():\n",
    "                self.feature_ll[label][word] = math.log(count\n",
    "                                                        / (self.class_total_feature_freq[label] - count))\n",
    "        print(f\"\\nLog likelihood ratio of each feature for each class: \\n\", self.feature_ll)\n",
    "\n",
    "    def test(self, tokenized_document: list):\n",
    "        \"\"\"\n",
    "        Classifies a single text document.\n",
    "        A continuation of the implementation inspired by the Multinomial Naive Bayes algorithm presented in\n",
    "        Speech and Language Processing (Jurafsky & Martin, 2022), page 62, Figure 4.2.\n",
    "        :param tokenized_document: Test document on the form of a single list of lemmatized tokens.\n",
    "        :return: A string with the predicted classification\n",
    "        \"\"\"\n",
    "        smoothing_values = {}\n",
    "        # Retrieves the priors for the genres and saves them to a new variable\n",
    "        for label in self.classes:\n",
    "            self.sum_ll[label] = self.priorlogs[label]\n",
    "\n",
    "            # Values for smoothing the probabilities when words in the test document aren't found in the current genre,\n",
    "            # but are found in a different one\n",
    "            smoothing_values[label] = math.log(1 / (self.class_total_feature_freq[label] + self.total_doc_count))\n",
    "\n",
    "        # Loops over the feature classes and the words in the input document\n",
    "        # and calculates the sums of the log likelihoods for each class\n",
    "        for current_class in self.classes:\n",
    "            for word in tokenized_document:\n",
    "                # If the word has occurred in training data for current class\n",
    "                if word in self.feature_ll[current_class]:\n",
    "                    # Add its log likelihood to the sum of likelihoods for the current class\n",
    "                    self.sum_ll[current_class] += self.feature_ll[current_class][word]\n",
    "                # Else, check if the word has occurred in the training data for a different class\n",
    "                else:\n",
    "                    other_classes = [label for label in self.classes if label != current_class]\n",
    "                    for other_class in other_classes:\n",
    "                        # If it does, smooth the probability for the current class\n",
    "                        if word in self.feature_ll[other_class]:\n",
    "                            self.sum_ll[current_class] += smoothing_values[current_class]\n",
    "        print(f\"Sum of log likelihoods for D = {tokenized_document}: \\n\", self.sum_ll, \"\\n\\n\")\n",
    "\n",
    "        # Determines the test document class by selecting the class with the maximum log likelihood\n",
    "        return max(self.sum_ll, key=self.sum_ll.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5a5f1",
   "metadata": {},
   "source": [
    "### Testing the Multinomial Naive Bayes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4dffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = [\n",
    "    [\"fun\", \"couple\", \"love\", \"love\"],\n",
    "    [\"fast\", \"furious\", \"shoot\"],\n",
    "    [\"couple\", \"fly\", \"fast\", \"fun\", \"fun\"], \n",
    "    [\"furious\", \"shoot\", \"shoot\", \"fun\"],\n",
    "    [\"fly\", \"fast\", \"shoot\", \"love\"]\n",
    "]\n",
    "y_train = [\"comedy\", \"action\", \"comedy\", \"action\", \"action\"]\n",
    "\n",
    "# Test data (two different samples)\n",
    "X_test_1 = [\"fast\", \"couple\", \"shoot\", \"fly\"]\n",
    "X_test_2 = [\"fast\", \"couple\", \"love\", \"furious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac638f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes:  ['comedy', 'action']\n",
      "Total number of documents:  5\n",
      "Training sample counts per class:  dict_items([('comedy', {'fun': 5, 'couple': 4, 'love': 4, 'fly': 3, 'fast': 3}), ('action', {'fast': 4, 'furious': 4, 'shoot': 6, 'fun': 3, 'fly': 3, 'love': 3})])\n",
      "comedy total feature counts: 19\n",
      "Log prior for comedy:  1.33500106673234\n",
      "action total feature counts: 23\n",
      "Log prior for action:  1.5260563034950492\n",
      "\n",
      "Log likelihood ratio of each feature for each class: \n",
      " defaultdict(<class 'dict'>, {'comedy': {'fun': -1.0296194171811581, 'couple': -1.3217558399823195, 'love': -1.3217558399823195, 'fly': -1.6739764335716716, 'fast': -1.6739764335716716}, 'action': {'fast': -1.55814461804655, 'furious': -1.55814461804655, 'shoot': -1.041453874828161, 'fun': -1.8971199848858813, 'fly': -1.8971199848858813, 'love': -1.8971199848858813}})\n"
     ]
    }
   ],
   "source": [
    "# Building the classifier\n",
    "nb = MultinomialNaiveBayes()\n",
    "nb.train(X_train, y_train, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b24de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of log likelihoods for D = ['fast', 'couple', 'shoot', 'fly']: \n",
      " {'comedy': -6.512761470741268, 'action': -6.302866684440747} \n",
      "\n",
      "\n",
      "Sum of log likelihoods for D = ['fast', 'couple', 'love', 'furious']: \n",
      " {'comedy': -6.160540877151917, 'action': -6.819557427659136} \n",
      "\n",
      "\n",
      "Prediction for D = ['fast', 'couple', 'shoot', 'fly'] is action\n",
      "Prediction for D = ['fast', 'couple', 'love', 'furious'] is comedy\n"
     ]
    }
   ],
   "source": [
    "# Running predictions on two different test samples\n",
    "y_pred_1 = nb.test(X_test_1)\n",
    "y_pred_2 = nb.test(X_test_2)\n",
    "print(f\"Prediction for D = {X_test_1} is {y_pred_1}\")\n",
    "print(f\"Prediction for D = {X_test_2} is {y_pred_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045808c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes:  ['comedy', 'horror', 'drama', 'action']\n",
      "Total number of documents:  9\n",
      "Training sample counts per class:  dict_items([('comedy', {'fun': 4, 'couple': 3, 'love': 3, 'fly': 2, 'fast': 2}), ('action', {'fast': 3, 'furious': 3, 'shoot': 5, 'fun': 2, 'fly': 2, 'love': 2}), ('drama', {'slow': 2, 'couple': 2, 'emotional': 3, 'impactful': 2, 'love': 3, 'great': 2, 'story': 2, 'historic': 2}), ('horror', {'tense': 3, 'scary': 2, 'frightening': 2, 'fast': 2, 'shoot': 2, 'dark': 3, 'horrific': 2, 'kill': 2, 'clown': 2, 'sewer': 2})])\n",
      "comedy total feature counts: 14\n",
      "Log prior for comedy:  0.44183275227903923\n",
      "horror total feature counts: 22\n",
      "Log prior for horror:  0.8938178760220965\n",
      "drama total feature counts: 18\n",
      "Log prior for drama:  0.6931471805599453\n",
      "action total feature counts: 17\n",
      "Log prior for action:  0.6359887667199967\n",
      "\n",
      "Log likelihood ratio of each feature for each class: \n",
      " defaultdict(<class 'dict'>, {'comedy': {'fun': -0.916290731874155, 'couple': -1.2992829841302609, 'love': -1.2992829841302609, 'fly': -1.791759469228055, 'fast': -1.791759469228055}, 'horror': {'tense': -1.845826690498331, 'scary': -2.3025850929940455, 'frightening': -2.3025850929940455, 'fast': -2.3025850929940455, 'shoot': -2.3025850929940455, 'dark': -1.845826690498331, 'horrific': -2.3025850929940455, 'kill': -2.3025850929940455, 'clown': -2.3025850929940455, 'sewer': -2.3025850929940455}, 'drama': {'slow': -2.0794415416798357, 'couple': -2.0794415416798357, 'emotional': -1.6094379124341003, 'impactful': -2.0794415416798357, 'love': -1.6094379124341003, 'great': -2.0794415416798357, 'story': -2.0794415416798357, 'historic': -2.0794415416798357}, 'action': {'fast': -1.540445040947149, 'furious': -1.540445040947149, 'shoot': -0.8754687373538999, 'fun': -2.0149030205422647, 'fly': -2.0149030205422647, 'love': -2.0149030205422647}})\n"
     ]
    }
   ],
   "source": [
    "# Re-training on a multi-class dataset\n",
    "# Training data\n",
    "X_train = [\n",
    "    [\"fun\", \"couple\", \"love\", \"love\"],\n",
    "    [\"fast\", \"furious\", \"shoot\"],\n",
    "    [\"couple\", \"fly\", \"fast\", \"fun\", \"fun\"], \n",
    "    [\"furious\", \"shoot\", \"shoot\", \"fun\"],\n",
    "    [\"fly\", \"fast\", \"shoot\", \"love\"],\n",
    "    [\"slow\", \"couple\", \"emotional\", \"impactful\", \"love\"],\n",
    "    [\"great\", \"emotional\", \"love\", \"story\", \"historic\"],\n",
    "    [\"tense\", \"scary\", \"frightening\", \"fast\", \"shoot\", \"dark\"],\n",
    "    [\"horrific\", \"tense\", \"dark\", \"kill\", \"clown\", \"sewer\"]\n",
    "\n",
    "]\n",
    "y_train = [\"comedy\", \"action\", \"comedy\", \"action\", \"action\", \"drama\", \"drama\", \"horror\", \"horror\"]\n",
    "\n",
    "# Building the classifier\n",
    "nb = MultinomialNaiveBayes()\n",
    "nb.train(X_train, y_train, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e67d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of log likelihoods for D = ['fast', 'couple', 'shoot', 'fly']: \n",
      " {'comedy': -10.71195760216563, 'horror': -17.447301127906577, 'drama': -24.457152423150195, 'action': -10.311021108166281} \n",
      "\n",
      "\n",
      "Sum of log likelihoods for D = ['fast', 'couple', 'love', 'furious']: \n",
      " {'comedy': -7.083986901138687, 'horror': -22.012690443882825, 'drama': -16.179079737571307, 'action': -10.975997411759531} \n",
      "\n",
      "\n",
      "Sum of log likelihoods for D = ['scary', 'historic', 'bad', 'futuristic', 'dark']: \n",
      " {'comedy': -8.96464989550841, 'horror': -6.688581111955426, 'drama': -7.977968093128549, 'action': -9.13830084734445} \n",
      "\n",
      "\n",
      "Prediction for D = ['fast', 'couple', 'shoot', 'fly'] is action\n",
      "Prediction for D = ['fast', 'couple', 'love', 'furious'] is comedy\n",
      "Prediction for D = ['scary', 'historic', 'bad', 'futuristic', 'dark'] is horror\n"
     ]
    }
   ],
   "source": [
    "# Test data (three different samples)\n",
    "X_test_1 = [\"fast\", \"couple\", \"shoot\", \"fly\"]\n",
    "X_test_2 = [\"fast\", \"couple\", \"love\", \"furious\"]\n",
    "X_test_3 = [\"scary\", \"historic\", \"bad\", \"futuristic\", \"dark\"]\n",
    "\n",
    "# Running predictions on two different test samples\n",
    "y_pred_1 = nb.test(X_test_1)\n",
    "y_pred_2 = nb.test(X_test_2)\n",
    "y_pred_3 = nb.test(X_test_3)\n",
    "\n",
    "print(f\"Prediction for D = {X_test_1} is {y_pred_1}\")\n",
    "print(f\"Prediction for D = {X_test_2} is {y_pred_2}\")\n",
    "print(f\"Prediction for D = {X_test_3} is {y_pred_3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e13d24989e9aa84deaafec43a9be4b3b9f46f3756b11886b1eace5ab5637549"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
