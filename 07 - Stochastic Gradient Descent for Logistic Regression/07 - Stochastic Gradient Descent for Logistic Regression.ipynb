{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1e7e1b",
   "metadata": {},
   "source": [
    "---\n",
    "# Stochastic Gradient Descent for Logistic Regression\n",
    "### An assignment as part of the course INF367: Selected Topics in Artificial Intelligence at the University of Bergen\n",
    "\n",
    "### Description of the problem\n",
    "We want to discover the relationship between the features of some observation(s) and some outcome, e.g. a classification. Given enough samples of sufficient quality, we can train a probabilistic machine learning classifier to predict the outcome for new samples. Such a classifier generally has four components: \n",
    "\n",
    "1. A feature representation of the input. \n",
    "    For each input observation $x^i$, this will be a vector of features $[x_1 , x_2 , ..., x_n]$.\n",
    "2. A classification function that computes $ŷ$, the estimated class, via $p(y|x)$. \n",
    "3. An objective function for learning, usually involving minimizing error on\n",
    "training examples.\n",
    "4. An algorithm for optimizing the objective function. <br>\n",
    "(Jurafsky & Martin, 2022)\n",
    "\n",
    "This report will take a closer look at an algorithm for component 4, namely stochastic gradient descent for binary logistic regression. \n",
    "\n",
    "\n",
    "### Description of the approach\n",
    "This report provides and describes an implementation of the stochastic gradient descent algorithm detailed on page 93 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf), Third Edition, by Daniel Jurafsky and James H. Martin, which can be seen in the figure below:\n",
    "\n",
    "<div>\n",
    "<img src=\"sgd_alg_jurafsky.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\"><i><b>Fig. 3</b>: The Stochastic Gradient Descent Algorithm. (Jurafsky & Martin, 2022)<br></i></div>\n",
    "\n",
    "_Step 1 (computing the loss) is used mainly to report how well we are doing on the current tuple; we don’t need to compute the loss in order to compute the gradient. The algorithm can terminate when it converges (or when the gradient norm $<\\epsilon$), or when progress halts (for example when the loss starts going up on a held-out set)._ (Jurafsky & Martin, 2022)\n",
    "\n",
    "My approach differs somewhat from the description above. The main difference is that my implementation of stochastic gradient descent doesn't take the loss function $L(f(x; \\theta), y)$\n",
    "    and the function parametrized by $\\theta$ $f(x, \\theta)$ as input. \n",
    "    Since this implementation is for binomial logistic regression, I've added $L$ and $f$ into the class methods.\n",
    "    The cross-entropy loss function $L(\\sigma(w \\cdot x + b) - y) * x^i)$ is part of the `gradient` method,\n",
    "    while the function parametrized by $\\theta$ is the sigmoid function implemented in the `sigmoid` method.\n",
    "\n",
    "### Description of the software\n",
    "There are two ways to run the software: <br>\n",
    "**1. As part of this Jupyter Notebook.** <br>\n",
    "Open this file in Jupyter Notebook or an IDE capable of running notebooks (e.g. PyCharm). Select \"Kernel\" from the top menu, then \"Restart and Run All\" to run the notebook in your local environment. \n",
    "Since there are no dependencies other than `numpy` required to run the code, re-use through copy-pasting the code is also trivial. Simply copy-paste the code over to your Python file and you should be able to run it. \n",
    "**2. By pip installation** <br>\n",
    "A directory called `nb_sgd_classify` has been provided along with this notebook. To install the software, simply download the folder, navigate to it in the terminal, and run the command `python3 -m pip install .` (punctuation included. Indicates that pip should install the current directory.) This will install both the software package and required dependencies (numpy), unless these are already installed. Then you may use the SGD and Naive Bayes classes as you wish by importing them as usual. See the `test` directory for example usage. \n",
    "\n",
    "<br>\n",
    "My version of the stochastic gradient descent algorithm is implemented as a Python class with three methods: <br>\n",
    "1. `sigmoid`: The logistic function. <br>\n",
    "2. `gradient`: Calculates the loss, and the gradient of the loss function. <br>\n",
    "3. `train`: Runs the gradient descent algorithm on the training data. <br>\n",
    "\n",
    "The class does not have a test method, because this wasn't required in the assignment text, and I ran out of time. I do, however, aim to implement a test method in the future as part of a complete implementation of a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04efad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dd751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    An implementation of the stochastic gradient descent algorithm for binomial logistic regression.\n",
    "    \n",
    "    I used the algorithm description in 'Speech and Language Processing' (Jurafsky & Martin, 2022), page 93,\n",
    "    as a guideline for this implementation. \n",
    "    \n",
    "    The main difference between this algorithm and the one outlined\n",
    "    by Jurafsky & Martin is that this algorithm doesn't take the loss function 'L(f(x; theta), y)'\n",
    "    and the function parametrized by theta 'f(x, theta)' as input. Since this implementation\n",
    "    is for binomial logistic regression, I've added L and f into the class methods.\n",
    "    The cross-entropy loss function L (sigmoid(w.dot(x) + b) - y) * x[i]) is part of the gradient() method,\n",
    "    while the function parametrized by theta is the sigmoid function implemented in the sigmoid() method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.theta = None\n",
    "        self.b = 0\n",
    "        self.w = None\n",
    "        self.random_state = random_state\n",
    "        self.epoch = 0\n",
    "\n",
    "    @classmethod\n",
    "    def sigmoid(cls, z: float):\n",
    "        \"\"\"\n",
    "        The logistic function.\n",
    "        Maps a real value to the range [0, 1].\n",
    "        Used to calculate the gradient during gradient descent.\n",
    "        :param z: A floating point (real) value\n",
    "        :return: A floating point value in the range [0, 1].\n",
    "        \"\"\"\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "\n",
    "    @classmethod\n",
    "    def gradient(cls, x: np.ndarray, y: Union[int, float], theta: np.ndarray):\n",
    "        \"\"\"\n",
    "        Calculates the loss, and the gradient of the loss function.\n",
    "        :param x: A numpy array. Represents a single training sample.\n",
    "        :param y: A scalar. Represents the label for the training sample (0 or 1).\n",
    "        :param theta: A numpy array. Represents the current values for the weights.\n",
    "        :return: A numpy array of size len(x) + 1. Represents the gradient of the training sample and the bias.\n",
    "        \"\"\"\n",
    "        # Extracts the weights from the parameter vector\n",
    "        w = np.delete(theta, -1)\n",
    "        # Extracts the bias from the parameter vector\n",
    "        b = theta[-1]\n",
    "        # Calculates the loss for each weight and then calculates the derivative\n",
    "        delta_w = np.array([(cls.sigmoid(w.dot(x) + b) - y) * x[i] for i in range(len(x))])\n",
    "        # Calculates the loss for the bias and then calculates the derivative\n",
    "        delta_b = cls.sigmoid(w.dot(x) + b) - y\n",
    "        # Extends the derivative of the bias with the derivative of the weights\n",
    "        delta = np.append(delta_w, delta_b)\n",
    "        # Returns the derivative (gradient) of the (entire) loss function\n",
    "        return delta\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, learning_rate: float = 0.01, max_iter: int = 1000, tolerance: float = 0.0001):\n",
    "        \"\"\"\n",
    "        Runs the gradient descent algorithm on the training data.\n",
    "        :param X_train: A numpy array of arrays, where each subarray contains the feature values\n",
    "                        for a single training sample.\n",
    "        :param y_train: A numpy array containing the values for the label of each training sample.\n",
    "        :param learning_rate: The value of the learning rate hyperparameter,\n",
    "                              a.k.a. the \"step size\" for gradient descent.\n",
    "        :param max_iter: The maximum number of epochs (passes/iterations) to performed during gradient descent.\n",
    "        :param tolerance: The minimum\n",
    "        :return: Theta, i.e. the optimal parameters for the weights w and the bias b.\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic error handling for user input\n",
    "        if len(X_train) == 0:\n",
    "            raise ValueError(\"X_train must have at least 1 sample. \")\n",
    "        if len(X_train) != len(y_train):\n",
    "            raise ValueError(\"X_train and y_train must be of the same length.\")\n",
    "        if learning_rate <= 0 or max_iter <= 0 or tolerance <= 0:\n",
    "            raise ValueError(\"The hyperparameters 'learning_rate', 'max_iter' and 'tolerance' must be greater than 0.\")\n",
    "\n",
    "        # Initializes the weights and the bias to 0\n",
    "        self.theta = np.array([0 for i in range(len(X_train[0]))] + [self.b])\n",
    "        print(f\"Number of parameters to tune: {len(self.theta)}\")\n",
    "\n",
    "        # Randomly selects a set of indices of training samples to use for SGD algorithm\n",
    "        random.seed(self.random_state)\n",
    "        random_indices = random.sample([i for i in range(len(X_train))], len(X_train))\n",
    "\n",
    "        # Performs gradient descent with max_iter number of steps towards the optimum\n",
    "        for epoch in range(max_iter + 1):\n",
    "            self.epoch += 1\n",
    "            # Randomly selects a sample\n",
    "            for i in random_indices:\n",
    "                # Calculates the gradient\n",
    "                delta = self.gradient(X_train[i], y_train[i], self.theta)\n",
    "                # Checks whether the change in weights will be greater than the tolerance\n",
    "                if np.all(np.abs(learning_rate * delta) <= tolerance):\n",
    "                    # If it isn't, stop the gradient descent and return the parameters tuned after the previous epoch\n",
    "                    print(\"\\n\", f\"\"\"\n",
    "                    The optimal parameters after {self.epoch} epochs are:\n",
    "                    w1 = {self.theta[0]}\n",
    "                    w2 = {self.theta[1]}\n",
    "                    b = {self.theta[2]}\n",
    "                    \"\"\")\n",
    "                    return self.theta\n",
    "                # Updates the weights\n",
    "                else:\n",
    "                    self.theta = self.theta - (learning_rate * delta)\n",
    "            if epoch == 0 or (epoch + 1)% 10 == 0:\n",
    "                print(f\"Parameters after epoch {epoch + 1}: {self.theta}\")\n",
    "\n",
    "        print(\"\\n\", f\"\"\"\n",
    "        The optimal parameters after {self.epoch} epochs are:\n",
    "        w1 = {self.theta[0]}\n",
    "        w2 = {self.theta[1]}\n",
    "        b = {self.theta[2]}\n",
    "        \"\"\")\n",
    "        \n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3dd77f",
   "metadata": {},
   "source": [
    "### Testing the implementation on a single training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba745c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters to tune: 3\n",
      "Parameters after epoch 1: [0.015 0.01  0.005]\n",
      "Parameters after epoch 10: [0.12873805 0.08582537 0.04291268]\n",
      "Parameters after epoch 20: [0.22152599 0.14768399 0.073842  ]\n",
      "Parameters after epoch 30: [0.2917358  0.19449053 0.09724527]\n",
      "Parameters after epoch 40: [0.3473211 0.2315474 0.1157737]\n",
      "Parameters after epoch 50: [0.39294022 0.26196014 0.13098007]\n",
      "Parameters after epoch 60: [0.43143619 0.28762413 0.14381206]\n",
      "Parameters after epoch 70: [0.46463256 0.30975504 0.15487752]\n",
      "Parameters after epoch 80: [0.49375382 0.32916922 0.16458461]\n",
      "Parameters after epoch 90: [0.51965545 0.34643697 0.17321848]\n",
      "Parameters after epoch 100: [0.54295553 0.36197035 0.18098518]\n",
      "Parameters after epoch 110: [0.56411354 0.37607569 0.18803785]\n",
      "Parameters after epoch 120: [0.58347949 0.38898632 0.19449316]\n",
      "Parameters after epoch 130: [0.60132559 0.40088373 0.20044186]\n",
      "Parameters after epoch 140: [0.61786746 0.41191164 0.20595582]\n",
      "Parameters after epoch 150: [0.63327855 0.4221857  0.21109285]\n",
      "Parameters after epoch 160: [0.64770038 0.43180026 0.21590013]\n",
      "Parameters after epoch 170: [0.66124982 0.44083322 0.22041661]\n",
      "Parameters after epoch 180: [0.67402446 0.44934964 0.22467482]\n",
      "Parameters after epoch 190: [0.68610655 0.45740437 0.22870218]\n",
      "Parameters after epoch 200: [0.69756608 0.46504405 0.23252203]\n",
      "Parameters after epoch 210: [0.70846305 0.4723087  0.23615435]\n",
      "Parameters after epoch 220: [0.71884926 0.47923284 0.23961642]\n",
      "\n",
      " \n",
      "                    The optimal parameters after 224 epochs are:\n",
      "                    w1 = 0.7218724187718505\n",
      "                    w2 = 0.4812482791812345\n",
      "                    b = 0.24062413959061724\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "# Instantiating and running Stochastic Gradient Descent on a single training example\n",
    "X_train = np.array([[3, 2]])  # A single training example\n",
    "y_train = np.array([1])  # The label for our training example (1 = \"positive sentiment\", 0 = \"negative sentiment\")\n",
    "sgd = SGD(random_state=42)\n",
    "theta = sgd.train(X_train, y_train, learning_rate=0.01, max_iter=500, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cadbf9",
   "metadata": {},
   "source": [
    "### Testing the implementation on a multiple training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675f92f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters to tune: 3\n",
      "Parameters after epoch 1: [ 0.03255003 -0.10212835  0.00107459]\n",
      "Parameters after epoch 10: [ 0.33036689 -0.43561577  0.06030454]\n",
      "Parameters after epoch 20: [ 0.51664447 -0.61571864  0.10372972]\n",
      "\n",
      " \n",
      "                    The optimal parameters after 23 epochs are:\n",
      "                    w1 = 0.5661911317529388\n",
      "                    w2 = -0.6359628022501363\n",
      "                    b = 0.11921399347902256\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "# Instantiating and running Stochastic Gradient Descent on a multiple training examples\n",
    "X_train = np.array([[3, 2], [4, 1], [5, 0], [2, 1], [1, 0], [0, 1], [3, 6], [2 ,8], [3, 3], [1, 8]])\n",
    "y_train = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])  # The label for our training examples\n",
    "sgd = SGD(random_state=42)\n",
    "theta = sgd.train(X_train, y_train, learning_rate=0.01, max_iter=500, tolerance=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e13d24989e9aa84deaafec43a9be4b3b9f46f3756b11886b1eace5ab5637549"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
